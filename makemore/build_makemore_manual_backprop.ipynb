{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ba9446",
   "metadata": {},
   "source": [
    "## Makemore Part 4: becoming a backprop ninja\n",
    "Manually coding our backpropagation \n",
    "https://www.youtube.com/watch?v=q8SA3rM6ckI&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=5&pp=iAQB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c1572c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run using python 3.8.5 kernel \n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9bef5828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines() \n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "366e178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build vocab of chars and mapping to / from integers\n",
    "chars = sorted(list(set(''.join(words)))) # get all distinct chars, sorted\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0 \n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "66db6fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build dataset\n",
    "block_size = 3 \n",
    "\n",
    "def build_dataset(words): \n",
    "\n",
    "    X, Y = [], [] \n",
    "\n",
    "    for w in words: \n",
    "        context = [0] * block_size \n",
    "        for ch in w + '.': \n",
    "            ix = stoi[ch]  \n",
    "            X.append(context) \n",
    "            Y.append(ix) \n",
    "            context = context[1:] + [ix] \n",
    "\n",
    "    X = torch.tensor(X) \n",
    "    Y = torch.tensor(Y) \n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y \n",
    "\n",
    "import random \n",
    "random.seed(42) \n",
    "random.shuffle(words) # mix up the words \n",
    "n1 = int(0.8*len(words)) # 80% of words \n",
    "n2 = int(0.9*len(words)) # 90% of words \n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) \n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d84c699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function he created \n",
    "# checking our gradients vs. pytorch's gradients: are we correct? \n",
    "def cmp(s, dt, t): \n",
    "    ex = torch.all(dt == t.grad).item() # are they exactly equal? \n",
    "    app = torch.allclose(dt, t.grad) # are they approximately equal according to allclose? \n",
    "    maxdiff = (dt - t.grad).abs().max().item() # biggest diff between our grads and pytorch's\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f327fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 \n",
    "n_hidden = 64 \n",
    "\n",
    "## Note that these are initialized in non-standard ways bc otherwise our \n",
    "# backprop could be too easy\n",
    "# He also notes b1 is spurious here, but I would've thought both b2?  \n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),               generator=g)\n",
    "# Layer 1 \n",
    "W1 = torch.randn((n_embd * block_size, n_hidden),   generator=g) * (5/3)/((n_embd*block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                          generator=g) * 0.1\n",
    "# Layer 2 \n",
    "W2 = torch.randn((n_hidden, vocab_size),            generator=g) * 0.1 \n",
    "b2 = torch.randn(vocab_size,                        generator=g) * 0.1 \n",
    "# Batchnorm parameters \n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0 \n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1 \n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters: \n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2a18e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "n = batch_size # shorter variable also for convenience \n",
    "\n",
    "# constructing one minibatch \n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "39cbc998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3126, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, but chunked into smaller steps that we can backward one at a time \n",
    "\n",
    "emb = C[Xb] # embed chars into vectors \n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors \n",
    "\n",
    "# Linear layer 1 \n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation (i.e. pre tanh etc.)\n",
    "# Batchnorm layer \n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani \n",
    "bndiff2 = bndiff**2 \n",
    "bnvar = 1/(n-1) * (bndiff2).sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv \n",
    "hpreact = bngain * bnraw + bnbias \n",
    "# Non-linearity (activation) \n",
    "h = torch.tanh(hpreact) # hidden layer \n",
    "\n",
    "# Linear layer 2 \n",
    "logits = h @ W2 + b2 \n",
    "# cross entropy loss \n",
    "logit_maxes = logits.max(1, keepdim=True).values \n",
    "# subtract max for numerical stability -- don't want exp to overflow\n",
    "# this won't affect the logprobs down the line bc we subtract the same amt from everything\n",
    "norm_logits = logits - logit_maxes \n",
    "counts = norm_logits.exp() \n",
    "counts_sum = counts.sum(1, keepdims=True) \n",
    "counts_sum_inv = counts_sum**-1 \n",
    "probs = counts * counts_sum_inv \n",
    "logprobs = probs.log() \n",
    "loss = -logprobs[range(n), Yb].mean() \n",
    "\n",
    "# Pytorch backward pass \n",
    "for p in parameters: \n",
    "    p.grad = None \n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, \n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw, \n",
    "          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, \n",
    "          embcat, emb]: \n",
    "    t.retain_grad() \n",
    "loss.backward() \n",
    "loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04842386",
   "metadata": {},
   "source": [
    "## Exercise 1: backprop thru the whole thing manually \n",
    "## backpropagating as defined above, one by one\n",
    "Example: \n",
    "dloss / da is -1/n for all the correct element indexes (e.g. if for example 1 \n",
    "char 5 is correct, gradient for that element is -1/n bc loss is negative mean \n",
    "of logprobs)\n",
    "the other logprob values don't participate / affect the loss  \n",
    "\n",
    "Example 2: \n",
    "dprobs is the local derivative of logprobs w/ respect to probs, multiplied by \n",
    "derivative of loss w respect to logprobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2a8a0ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs) \n",
    "dlogprobs[range(n), Yb] = -1.0 / n \n",
    "\n",
    "# Note this one super logical bc if prob is very high like 1, dlogprobs just \n",
    "# gets passed through. If prob (of correct value) is low that's bad, so \n",
    "# gradient gets boosted (we need more change) \n",
    "dprobs = (1.0/probs) * dlogprobs \n",
    "\n",
    "# We have to sum here bc counts_sum_inv is actually not the same dimension \n",
    "# as counts -- it's a column that gets replicated 27 times. And the node \n",
    "# accumulates all those gradients (or can think of it as all that power over L) -- \n",
    "# we need to sum that up \n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True) \n",
    "# this one 32x1 and 32x27, counts_sum_inv will broadcast to get correct result \n",
    "dcounts = counts_sum_inv * dprobs \n",
    "dcounts_sum = -counts_sum**-2 * dcounts_sum_inv \n",
    "\n",
    "# note here that dcounts_sum is one column; dcounts is 32x27 \n",
    "# we want to take a 1-column derivative and transform it into a 32x27 derivative \n",
    "# and sum means that each element has local derivative 1 \n",
    "# **Note also this is a second branch of dcounts gradient! Need to add it to the above one! \n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "# nice and simple bc it's element-wise\n",
    "# (counts here is norm_logits.exp(), we're just reusing the calculation)\n",
    "dnorm_logits = counts * dcounts  \n",
    "\n",
    "# logit_maxes also getting broadcast here \n",
    "# basically can remember: if a vector is getting broadcast, you've gotta sum the gradients \n",
    "# I think of this as creating a 32x27 of -1s, then multiplying that by dnorm_logits, \n",
    "# and THEN adding those elements together \n",
    "dlogits = dnorm_logits.clone() # think of it as 1s * dnorm_logits\n",
    "dlogit_maxes = -dnorm_logits.sum(1, keepdim=True)\n",
    "\n",
    "# for the line logit_maxes = logit.max(1, keepdim=True).values \n",
    "# the gradient should be 1 for the entries that were 'plucked out' as the max in each row \n",
    "# for the rest, it should be 0 \n",
    "# and dlogit_maxes is broadcasting here \n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "# time to get into some matrices \n",
    "# basically, the derivative for c = a @ b is going to be b.T or a.T, \n",
    "# for a and b respectively. And if you look at the dims of the matrices, there's \n",
    "# only one way for it to work out with dc (a.T @ dc for example)\n",
    "# and the bias makes sense bc one row becomes many; you sum to eliminate the column dimension\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits \n",
    "db2 = dlogits.sum(0, keepdims=True)\n",
    "\n",
    "# backpropagating thru a tanh \n",
    "dhpreact = (1.0 - h**2) * dh \n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True) \n",
    "dbnraw = bngain * dhpreact \n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "dbndiff = bnvar_inv * dbnraw \n",
    "# nice to think of this sum also as a sum across examples \n",
    "# bc bias-type vars apply the same way to every example \n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = -0.5 * ((bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "\n",
    "# anytime there's a sum in the forwar pass, something will have to broadcast\n",
    "# in the backward pass. Here dbnvar is broadcasting \n",
    "# And vice versa! \n",
    "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar \n",
    "dbndiff += 2*bndiff * dbndiff2 # another branch of one we drafted -- thus += \n",
    "\n",
    "# broadcasting in forward pass = variable reuse = sum in backward pass \n",
    "dhprebn = dbndiff.clone() # he uses clones (exact copy) for \"safety\" \n",
    "dbnmeani = -dbndiff.sum(0, keepdim=True) \n",
    "dhprebn += (1.0/n) * torch.ones_like(hprebn) * dbnmeani \n",
    "\n",
    "# first layer \n",
    "dembcat = dhprebn @ W1.T \n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "\n",
    "demb = dembcat.view(emb.shape) # just reimagine embcat in the original shape of emb\n",
    "\n",
    "dC = torch.zeros_like(C) \n",
    "for k in range(Xb.shape[0]): \n",
    "    for j in range(Xb.shape[1]): \n",
    "        ix = Xb[k,j] # e.g., 3 for letter d \n",
    "        dC[ix] += demb[k,j] # deposit emb's gradient for the corresponding vector \n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d0894cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([32, 30]),\n",
       " torch.Size([30, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprebn.shape, embcat.shape, W1.shape, b1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e10c3",
   "metadata": {},
   "source": [
    "### Exercise 2: backprop thru cross entropy but all in one go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5cfeab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.364447593688965 diff: -2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "loss_fast = F.cross_entropy(logits, Yb) \n",
    "print(loss_fast.item(), 'diff:', (loss_fast-loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "474a0843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.2177499532699585e-09\n"
     ]
    }
   ],
   "source": [
    "# he showed some calculus. bit complex but simplifies down a lot \n",
    "dlogits = F.softmax(logits, 1) \n",
    "dlogits[range(n), Yb] -= 1 # gradient depends on whether element is the correct one or not \n",
    "dlogits /= n # loss is average of losses, gradient needs to be scaled down too \n",
    "\n",
    "cmp('logits', dlogits, logits) # tiny tiny off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "739740f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f97631863a0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "23bef7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0690, 0.0838, 0.0197, 0.0501, 0.0206, 0.0841, 0.0242, 0.0348, 0.0163,\n",
       "        0.0308, 0.0410, 0.0367, 0.0403, 0.0277, 0.0348, 0.0144, 0.0093, 0.0190,\n",
       "        0.0159, 0.0538, 0.0486, 0.0220, 0.0235, 0.0736, 0.0599, 0.0244, 0.0219],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0] # looking at first example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3b6ea3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0690,  0.0838,  0.0197,  0.0501,  0.0206,  0.0841,  0.0242,  0.0348,\n",
       "        -0.9837,  0.0308,  0.0410,  0.0367,  0.0403,  0.0277,  0.0348,  0.0144,\n",
       "         0.0093,  0.0190,  0.0159,  0.0538,  0.0486,  0.0220,  0.0235,  0.0736,\n",
       "         0.0599,  0.0244,  0.0219], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# think of this as force\n",
    "# we're gonna pull down on the probability of the wrong chars\n",
    "# and we're gonna pull up on the probability of the correct one \n",
    "# (correct one is the only negative one here, I think meaning loss decreases if we boost it)\n",
    "# our application of force depends on how incorrect our predictions were \n",
    "dlogits[0] * n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "161cbcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3132e-10, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overall repulsion/attraction is equal \n",
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5afb4b",
   "metadata": {},
   "source": [
    "### Exercise 3: backprop through batchnorm, all in one go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "717d6b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Need to debug this -- why getting such a big diff? \n",
    "# fixed now, had redefined bnbias above instead of dbnbias \n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ecf74c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6665, -1.5681,  0.8980,  ...,  2.0226, -0.5969,  0.3400],\n",
       "         [-1.4487,  0.0939, -1.0534,  ..., -0.5668,  0.1939, -0.7310],\n",
       "         [-0.7711,  1.1059,  1.4311,  ...,  0.4884, -1.0139,  0.3550],\n",
       "         ...,\n",
       "         [ 0.0949, -0.1989,  0.2809,  ..., -0.3989,  0.0875,  0.4894],\n",
       "         [-1.1292, -2.0728,  1.0126,  ...,  0.5216,  1.5801, -1.6328],\n",
       "         [ 1.9189, -0.2238, -0.0079,  ..., -0.7749, -1.9075, -0.6047]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[-0.6665, -1.5681,  0.8980,  ...,  2.0226, -0.5969,  0.3400],\n",
       "         [-1.4487,  0.0939, -1.0534,  ..., -0.5668,  0.1939, -0.7310],\n",
       "         [-0.7711,  1.1059,  1.4311,  ...,  0.4884, -1.0139,  0.3550],\n",
       "         ...,\n",
       "         [ 0.0949, -0.1989,  0.2809,  ..., -0.3989,  0.0875,  0.4894],\n",
       "         [-1.1292, -2.0728,  1.0126,  ...,  0.5216,  1.5801, -1.6328],\n",
       "         [ 1.9189, -0.2238, -0.0079,  ..., -0.7749, -1.9075, -0.6047]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact_fast, hpreact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c1cdc27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# our derivatives are exactly the same... then why our values so different? \n",
    "# pretty complicated to get here -- in parallel, applying a complex derivative formula \n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a797c",
   "metadata": {},
   "source": [
    "## Exercise 4: putting it all together\n",
    "redefining functions with manual backprop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a683706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "51dc0135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.7733\n",
      "  10000/ 200000: 2.1969\n",
      "  20000/ 200000: 2.3853\n",
      "  30000/ 200000: 2.4146\n",
      "  40000/ 200000: 2.0234\n",
      "  50000/ 200000: 2.4041\n",
      "  60000/ 200000: 2.3364\n",
      "  70000/ 200000: 2.0575\n",
      "  80000/ 200000: 2.3676\n",
      "  90000/ 200000: 2.1796\n",
      " 100000/ 200000: 1.9514\n",
      " 110000/ 200000: 2.3410\n",
      " 120000/ 200000: 2.0097\n",
      " 130000/ 200000: 2.4040\n",
      " 140000/ 200000: 2.3332\n",
      " 150000/ 200000: 2.1821\n",
      " 160000/ 200000: 2.0361\n",
      " 170000/ 200000: 1.8173\n",
      " 180000/ 200000: 2.0636\n",
      " 190000/ 200000: 1.9300\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000 \n",
    "lossi = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for i in range(max_steps): \n",
    "        # minibatch \n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "        # forward pass \n",
    "        emb = C[Xb] # embed chars into vectors \n",
    "        embcat = emb.view(emb.shape[0], -1) # concatenate the vectors \n",
    "\n",
    "        # Linear layer 1 \n",
    "        hprebn = embcat @ W1 + b1 # hidden layer pre-activation (i.e. pre tanh etc.)\n",
    "        # Batchnorm layer \n",
    "        bnmean = hprebn.mean(0, keepdim=True)\n",
    "        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "        bnraw = (hprebn - bnmean) * bnvar_inv \n",
    "        hpreact = bngain * bnraw + bnbias \n",
    "        # Non-linearity (activation) \n",
    "        h = torch.tanh(hpreact) # hidden layer \n",
    "        # Linear layer 2 \n",
    "        logits = h @ W2 + b2 \n",
    "        loss = F.cross_entropy(logits, Yb) \n",
    "\n",
    "        for p in parameters: \n",
    "            p.grad = None \n",
    "        # loss.backward()\n",
    "\n",
    "        # manual backprop \n",
    "        # first, logits\n",
    "        dlogits = F.softmax(logits, 1) \n",
    "        dlogits[range(n), Yb] -= 1 \n",
    "        dlogits /= n \n",
    "        # 2nd layer \n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits \n",
    "        db2 = dlogits.sum(0)\n",
    "        # tanh \n",
    "        dhpreact = (1.0 - h**2) * dh \n",
    "        # batchnorm \n",
    "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True) \n",
    "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "        dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "        # 1st layer \n",
    "        dembcat = dhprebn @ W1.T \n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        # embedding \n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C) \n",
    "        for k in range(Xb.shape[0]): \n",
    "            for j in range(Xb.shape[1]): \n",
    "                ix = Xb[k,j] # e.g., 3 for letter d \n",
    "                dC[ix] += demb[k,j] # deposit emb's gradient for the corresponding vector \n",
    "\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "        # update \n",
    "        lr = 0.1 if i < 100000 else 0.01 \n",
    "        for p, grad in zip(parameters, grads): \n",
    "            # p.data += -lr * p.grad # need to update\n",
    "            p.data += -lr * grad \n",
    "\n",
    "        if i % 10000 == 0: \n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "            lossi.append(loss.log10().item())\n",
    "\n",
    "        # if i >= 100: \n",
    "        #     break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "28653c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking our grads against pytorch \n",
    "# for p, g in zip(parameters, grads): \n",
    "#     cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "eecc009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end \n",
    "with torch.no_grad(): \n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1 \n",
    "    bnmean = hpreact.mean(0, keepdim=True) \n",
    "    bnvar = hpreact.var(0, keepdim=True,  unbiased=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0710809230804443\n",
      "val 2.1087210178375244\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss \n",
    "# looks good! similar to before \n",
    "@torch.no_grad() # this decorator disables gradient tracking \n",
    "def split_loss(split): \n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1) \n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # these are adjusted to use our calibrated batch norm\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias \n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39d44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junee.\n",
      "noworu.\n",
      "chel.\n",
      "maevaya.\n",
      "zanni.\n",
      "talitzelyana.\n",
      "alie.\n",
      "lob.\n",
      "haybetse.\n",
      "bran.\n",
      "henon.\n",
      "thellee.\n",
      "tavir.\n",
      "anne.\n",
      "elliet.\n",
      "keyden.\n",
      "jecir.\n",
      "klor.\n",
      "malyine.\n",
      "abelle.\n"
     ]
    }
   ],
   "source": [
    "# lookin decent \n",
    "g = torch.Generator().manual_seed(12345)  \n",
    "\n",
    "for _ in range(20): \n",
    "    out = []\n",
    "    context = [0] * block_size \n",
    "    while True: \n",
    "        emb = C[torch.tensor([context])]\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        hpreact = embcat @ W1 + b1 \n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias \n",
    "        h = torch.tanh(hpreact) \n",
    "        logits = h @ W2 + b2 \n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item() \n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix) \n",
    "        if ix == 0: \n",
    "            break \n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
