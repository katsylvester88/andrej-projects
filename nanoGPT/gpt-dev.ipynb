{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development notebook \n",
    "Walking through process intuitively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-01 18:30:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-06-01 18:30:23 (15.7 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all of shakespeare's works, concatenated \n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f: \n",
    "    text = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chars in dataset:  1115394\n"
     ]
    }
   ],
   "source": [
    "# 1M chars roughly \n",
    "print(\"number of chars in dataset: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# get unique characters, sorted\n",
    "# funny that 3 is the only number? \n",
    "chars = sorted(list(set(text))) \n",
    "vocab_size = len(chars) \n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from characters to integers \n",
    "# this is tokenizing! \n",
    "# Google uses \"SentencePiece\" https://github.com/google/sentencepiece\n",
    "# OpenAI uses tiktoken \n",
    "# Tradeoff is amount of context vs. size of vocabulary (we have small vocab but lots of context) \n",
    "stoi = {ch:i for i,ch in enumerate(chars)} # string to integer \n",
    "itos = {i:ch for i,ch in enumerate(chars)} # integer to string \n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encodes chars \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decodes numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 53, 53]\n",
      "hellooo\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hellooo\"))\n",
    "print(decode(encode(\"hellooo\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch # pytorch. Need to use python3.8, doesn't support 3.13 \n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train validation split \n",
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "val = data[n:] # need it to be able to generalize on the val set as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# We want transformer to be used to seeing any number of chars as input \n",
    "# Anything up to block size \n",
    "block_size = 8 # aka context size  \n",
    "x = train[:block_size]\n",
    "y = train[1:block_size+1]\n",
    "for t in range(block_size): \n",
    "    context = x[:t+1]   \n",
    "    target = y[t] # y[t] = x[t+1] by definition. always 1 step ahead \n",
    "    print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# different way of writing the above, more intuitive to me  \n",
    "block_size = 8 \n",
    "sample = train[:block_size+1]\n",
    "for t in range(block_size): \n",
    "    context = sample[:t+1]   \n",
    "    target = sample[t+1]  \n",
    "    print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) \n",
    "batch_size = 4 # how many text sequences we'll process in parallel \n",
    "block_size = 8 # max context length \n",
    "\n",
    "def get_batch(split):  \n",
    "    data = train if split == 'train' else val \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # random offsets into training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack = stack 1D tensors as rows\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "-----\n",
      "when input is [24] then target is 43\n",
      "when input is [24, 43] then target is 58\n",
      "when input is [24, 43, 58] then target is 5\n",
      "when input is [24, 43, 58, 5] then target is 57\n",
      "when input is [24, 43, 58, 5, 57] then target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] then target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] then target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] then target is 39\n",
      "when input is [44] then target is 53\n",
      "when input is [44, 53] then target is 56\n",
      "when input is [44, 53, 56] then target is 1\n",
      "when input is [44, 53, 56, 1] then target is 58\n",
      "when input is [44, 53, 56, 1, 58] then target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] then target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] then target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] then target is 1\n",
      "when input is [52] then target is 58\n",
      "when input is [52, 58] then target is 1\n",
      "when input is [52, 58, 1] then target is 58\n",
      "when input is [52, 58, 1, 58] then target is 46\n",
      "when input is [52, 58, 1, 58, 46] then target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] then target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] then target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] then target is 46\n",
      "when input is [25] then target is 17\n",
      "when input is [25, 17] then target is 27\n",
      "when input is [25, 17, 27] then target is 10\n",
      "when input is [25, 17, 27, 10] then target is 0\n",
      "when input is [25, 17, 27, 10, 0] then target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] then target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] then target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] then target is 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape) # 4 sets of length-8 text, randomly sampled from data\n",
    "print(xb) \n",
    "\n",
    "print('targets:')\n",
    "print(yb.shape) # same as the above but offset by 1 (+1) \n",
    "print(yb)\n",
    "\n",
    "print('-----')\n",
    "# all of our 32 training examples / observations\n",
    "for b in range(batch_size): # rows \n",
    "    for t in range(block_size): # examples within rows \n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t] # imo easier to remember yb[t] is the same as xb[t+1]\n",
    "        print(f\"when input is {context.tolist()} then target is {target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with a bigram language model -- not sure why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F \n",
    "torch.manual_seed(1337) \n",
    "\n",
    "# seems like for some torch things you need to create modules for it? \n",
    "class BigramLanguageModel(nn.Module): \n",
    "    def __init__(self, vocab_size): \n",
    "        super().__init__() \n",
    "        # rows are current token, columns are possible next token \n",
    "        # values of the Embedding (not sure why it's called an Embedding in this case?) \n",
    "        # are probabilities \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None): \n",
    "        # this will be (B,T,C)\n",
    "        # B = batch, T = time, C = channel \n",
    "        # in our case b is 4, t is 8, c is vocab size or 65 \n",
    "        # i.e. will work for whole batch at once  \n",
    "        logits = self.token_embedding_table(idx) \n",
    "\n",
    "        if targets is None: \n",
    "            loss = None \n",
    "        else: \n",
    "            # how well are we predicting next char based on logits?  \n",
    "            # annoyingly, cross_entropy expects (B,C,T)       \n",
    "            B, T, C = logits.shape \n",
    "            logits = logits.view(B*T, C) # stretching array out to be 2D \n",
    "            targets = targets.view(B*T) \n",
    "            loss = F.cross_entropy(logits, targets) \n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): \n",
    "        # idx = current context of some characters (some batch) \n",
    "        # goal: take (B,T) make it (B, T+1), (B, T+2), ... (B, T+max_new_tokens)\n",
    "        # max_new_tokens = how many more chars we want to generate \n",
    "        for _ in range(max_new_tokens): \n",
    "            # get predictions\n",
    "            # apparently calling self will call forward? \n",
    "            # that must be the way the nn.Module works\n",
    "            logits, loss = self(idx) \n",
    "            # focus only on last time step (newest char?) \n",
    "            logits = logits[:, -1, :] # becomes (B, C) \n",
    "            # apply softmax to get probs \n",
    "            probs = F.softmax(logits, dim=-1) # still (B,C) \n",
    "            # sample from distrn \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) --> 1 for each batch\n",
    "            # append sampled index to the running sequence \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx \n",
    "    \n",
    "m = BigramLanguageModel(vocab_size) \n",
    "logits, loss = m(xb,yb) \n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# Note that we should know ideal initialization = -ln(1/65) = 4.1ish \n",
    "# so there's some entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# Trying out generate \n",
    "\n",
    "# creating a 1x1 tensor with just a 0 inside \n",
    "# this will be how we kick-off generation. It's a newline char\n",
    "# reasonable place to start \n",
    "idx = torch.zeros((1,1), dtype = torch.long)\n",
    "\n",
    "# asking for 100 new tokens \n",
    "# indexing in bc generate works at batch level \n",
    "# we convert tensor to list and use our decode function \n",
    "print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist())) \n",
    "\n",
    "# right now it's untrained :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# only used stochastic gradient descent in makemore \n",
    "# need to research more \n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4571714401245117\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 \n",
    "for steps in range(10000): \n",
    "    # sample a batch of data \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss \n",
    "    logits, loss = m(xb, yb) \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fourthicus ithe\n",
      "T:\n",
      "TEROMiaree Wisursethared k I'sprs\n",
      "Thofr bup indorther wee asthere t bofis p, ces\n",
      "omave me:\n",
      "I I tche:\n",
      "\n",
      "Y l:\n",
      "A wathigscay ain s sets u ar:\n",
      "It\n",
      "LIClorke ghe nd\n",
      "\n",
      "Morean.\n",
      "TESThind hert ai\n"
     ]
    }
   ],
   "source": [
    "# still really bad but definite progress \n",
    "print(decode(m.generate(idx=idx, max_new_tokens=200)[0].tolist())) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick in self-attention??? \n",
    "We want token 5 not to talk to 6, 7, 8; we do want it to talk to 1-4\n",
    "Info only flows in from the past (not future - we are trying to predict future) \n",
    "Simplest way to do that is avg the preceding tokens? \n",
    "Average channels from tokens 1-5 could summarize \"me\" (token 5) in context of history \n",
    "This is extremely lossy but we're accepting it for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy example \n",
    "torch.manual_seed(1337) \n",
    "B,T,C = 4,8,2 # batch, time, channels \n",
    "x = torch.randn(B, T, C) \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C)) # bow short for \"bag of words\" term for averaging \n",
    "for b in range(B): \n",
    "    for t in range(T): \n",
    "        xprev = x[b, :t+1] # everything up to and including this token \n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st row is same as 1st row above\n",
    "# 2nd row is avg of 1st and 2nd rows above\n",
    "# 3rd row is avg of 1-3 rows above etc\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# Mathematical trick: we can do this super quickly w matrix algebra \n",
    "# columns of C are like running sums of columns of b \n",
    "# easy to make this means rather than sums by just normalizing the rows of a \n",
    "torch.manual_seed(42) \n",
    "a = torch.tril(torch.ones(3,3)) # lower triangle of ones \n",
    "b = torch.randint(0, 10, (3,2)).float() \n",
    "c = a @ b \n",
    "print('a=')\n",
    "print(a) \n",
    "print('b=')\n",
    "print(b) \n",
    "print('--')\n",
    "print('c=')\n",
    "print(c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = a / torch.sum(a, 1, keepdim=True) \n",
    "c = a @ b \n",
    "print('a=')\n",
    "print(a) \n",
    "print('b=')\n",
    "print(b) \n",
    "print('--')\n",
    "print('c=')\n",
    "print(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here wei is (T,T) and x is (B, T, C) \n",
    "Pytorch will see the dimensions are different and since they align on right it will make wei into a batched application (B, T, T) and apply (T,T) to each batch. \n",
    "Then multiplying (T,T) @ (T,C) gives us (T,C) so we get (B,T,C) out \n",
    "Think of this as weighted aggregation as defined by wei! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True) \n",
    "xbow2 = wei @ x \n",
    "torch.allclose(xbow, xbow2) # the same! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3: use Softmax \n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# starting at 0 -- think of them as affinities \n",
    "# down the line, the affinities between certain characters will be stronger \n",
    "# \"how interesting they find each other\"\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # this line is saying, future tokens cannot be used \n",
    "# remember this exponentiates then divides by sum\n",
    "# so 0 --> 1, -inf --> 0 \n",
    "wei = F.softmax(wei, dim=-1) \n",
    "xbow3 = wei @ x \n",
    "torch.allclose(xbow, xbow3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to gather info from the past but in a data-dependent way \n",
    "* E.g. if I am a vowel, I may be interested in what consonants are in my past \n",
    "* And I want that data to flow to me \n",
    "\n",
    "THE WAY SELF ATTENTION DOES THIS \n",
    "\n",
    "Every single token will emit 2 vectors: a query and a key. \n",
    "Query vector: what am I looking for? \n",
    "Key vector: what do I contain? \n",
    "\n",
    "We basically do a dot product between the keys and the queries \n",
    "* The dot product becomes wei \n",
    "* If key and query are aligned, they will interact a lot, and you'll learn more \n",
    "about that specific token than others. \n",
    "* I believe that the fact that head size is 16 means there are 16 channels along which the tokens/nodes can communicate. So one channel might say \"I am a consonant!\" etc. \n",
    "* Then, thru softmax, that info that you're interested in will be aggregated and will influence how we predict the next char \n",
    "* Note also that rather than just aggregating x's, you actually aggregate each x's \"value\" channels. He says you can think of x as private info, value as public info: here is what I will tell you if you find me interesting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention \n",
    "torch.manual_seed(1337) \n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C) \n",
    "\n",
    "# single head performing self-attention \n",
    "head_size = 16 \n",
    "key = nn.Linear(C, head_size, bias=False) \n",
    "query = nn.Linear(C, head_size, bias=False) \n",
    "k = key(x) # (B,T,16) \n",
    "q = query(x) # also (B,T,16) \n",
    "value = nn.Linear(C, head_size, bias=False) \n",
    "wei = q @ k.transpose(-2, -1) # transpose last 2 dimensions. (B,T,16) @ (B,16,T) = (B,T,T) \n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1) \n",
    "\n",
    "v = value(x) # we use value rather than aggregating x exactly\n",
    "# can think of x as being \"private info\" \n",
    "# v is \"here is what I will communicate to you\"\n",
    "out = wei @ v # this is now 4x8x16 instead of 32 -- determined by head_size \n",
    "# out = wei @ x \n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, every batch element has different vals in wei\n",
    "wei "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other notes on attention: \n",
    "* Allows nodes to communicate with each other, in a data-dependent manner. In principle, this can be applied to any directed graph, although in our case we have a specific flow (e.g. node 1 can only point to itself, node 2 can only point to node 1 and node 2, etc)\n",
    "* In some cases your nodes may talk to each other more and not have this notion of some being off limits. E.g. sentiment analysis, you would want all tokens from the piece of text to communicate. In these cases, you would use an encoder block of self-attention -- basically deleting the part where we add -inf value to wei. (Ours is a decoder block apparently... this seems like vocab overload? bc we have a separate encoder and decoder too)\n",
    "* There is also no concept of space. By default they don't know where they are in the space -- that's why we created an embedding for their position (see positional embeddings in v2.py). This is distinct from convolutional neural networks (where filters act in space). \n",
    "* This is called self-attention bc keys, queries, and values are all coming from the same source: x. But in principle, attention is much more general. E.g. you could have queries produced from x, and keys and values coming from a different source. This is called cross-attention. \n",
    "* \"Scaled\" attention adds an important additional normalization. Divide by square root of head size. That seems to make sure that variance of wei is 1. If you scale the numbers in Wei up, softmax will lean more and more towards the max, and then each node is only learning from one other node (at worst).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
